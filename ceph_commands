http://download.ceph.com/tarballs/

http://docs.ceph.com/docs/master/man/8/ceph/


# show version
ceph version


# status
ceph -s


# health
ceph -w
ceph health


#  If you do not want CRUSH to automatically rebalance the cluster as you stop OSDs for maintenance, set the cluster to noout first
#  ( more flags to set: full|pause|noup|nodown|noout|noin|nobackfill|norebalance|norecover|noscrub|nodeep-scrub|notieragent|sortbitwise|require_jewel_osds )
ceph osd set noout
for i in noout nodown norebalance norecover nobackfill pause;do ceph osd set ${i};done
for i in noout nodown norebalance norecover nobackfill pause;do ceph osd unset ${i};done
#for i in noout nodown norebalance norecover noscrub nodeep-scrub ;do ceph osd set ${i};done
#for i in noout nodown norebalance norecover noscrub nodeep-scrub ;do ceph osd unset ${i};done

# restarting cluster members
for i in noout 
 

# list pools
ceph osd lspools


# disk usage
ceph df


# summarizes mds status
ceph mds stat -f json-pretty


# status of monitors
ceph mon_status -f json-pretty


# dump monitor state
ceph mon dump


# show osd tree
ceph osd tree


# list crush rule
ceph osd crush rule list -f json-pretty
ceph osd crush rule ls -f json-pretty


#shows current crush tunables
ceph osd crush show-tunables -f json-pretty


# shows the crush buckets and items in a tree view
ceph osd crush tree -f json-pretty


# sets crush tunables values to <profile>
ceph osd crush tunables legacy|argonaut|bobtail|firefly|hammer|optimal|default


# df shows OSD utilization
ceph osd df {plain|tree}


# get runtime config
# ceph daemon {daemon-type}.{id} config show | less
ceph daemon mon.cephconf01fra config show


# get config
ceph --admin-daemon /run/ceph/ceph-osd.0.asok config show | grep 'active'

# inject config
ceph tell {daemon-type}.{id or *} injectargs --{name} {value} [--{name} {value}]
ceph tell osd.0 injectargs --debug-osd 20 --debug-ms 1
ceph tell osd.* injectargs "--osd_recovery_max_active=3"

# nearfull ratio
ceph tell osd.* injectargs "--mon_osd_nearfull_ratio=.93"

# full ratio
ceph tell osd.* injectargs "--mon_osd_full_ratio=.98"


# allow deletion of pools
ceph tell mon.* injectargs "--mon_allow_pool_delete=true"


# Get the Number of Placement Groups
ceph osd pool get {pool-name} pg_num

# Get the Number of Placement Groups P
ceph osd pool get {pool-name} pgp_num {pgp_num}

# set pg_num and pgp_num
ceph osd pool set {pool-name} pg_num {pg_num}
ceph osd pool set {pool-name} pgp_num {pgp_num}


# Get Statistics for Stuck PGs
ceph pg dump_stuck inactive|unclean|stale|undersized|degraded [--format <format>] [<seconds>]
ceph pg dump_stuck unclean 5

# get pg map
ceph pg map 8.10a


http://docs.ceph.com/docs/master/rados/operations/crush-map/#warning-when-tunables-are-non-optimal

# if tunables are not optimal, ceph will complain ... requires at least kernel 4.x !
ceph osd crush tunables optimal
ceph osd crush tunables legacy




# remove host from bucket
# ceph osd crush remove {bucket-name}
ceph --cluster ceph-test osd crush remove $(hostname -s)



# list all "users"
ceph auth list


# get "user" caps
ceph auth get client.admin


# list erasure profiles
ceph osd erasure-code-profile ls


# import user
#The ceph storage cluster will add new users, their keys and their capabilities and will update existing users, their keys and their capabilities. 
ceph auth import -i /etc/ceph/ceph.keyring


# update client caps
ceph -c /etc/ceph/cephblocka01.conf auth caps client.pkgrepo mon 'allow r' mds 'allow' osd 'allow rwx pool=sftp_data'


# client caps
ceph auth get-or-create client.foo mon 'allow r' mds 'allow r, allow rw path=/mnt/' osd 'allow rw pool=sftp_data'
ceph auth get-or-create client.foo mon 'allow r' mds 'allow r' osd 'allow rw pool=sftp_data'


# ceph mds performance infos
ceph daemon mds.cephconf01fra perf dump mds


# after upgrade
ceph osd require-osd-release luminous


# set min version of clients
# jewel,kraken
ceph  osd set-require-min-compat-client luminous --yes-i-really-mean-it


# enable multimds
# 1st set the flag | 2nd set two active mds
ceph fs set FSNAME allow_multimds true
ceph fs set FSNAME max_mds 2

# deactivate ceph mds
# ceph mds deactivate CEPHFS:RANK
ceph mds deactivate cephdata:1

#mounting raw bluestore osds
ceph-objectstore-tool --op fuse --data-path /var/lib/ceph/osd/OSDNR--mountpount /mnt/foo


# memory usage of osds
ceph --cluster cephblocka01 daemon osd.6 dump_mempools


# block- or filestore ?
ceph --cluster cephblocka01 osd metadata $OSID | grep osd_objectstore


#current count of filestore vs bluestore
ceph --cluster cephblocka01 osd count-metadata osd_objectstore


# activate dashboard
ceph mgr module enable dashboard
ceph config-key set mgr/dashboard/$name/server_addr $IP
ceph config-key set mgr/dashboard/$name/server_port $PORT

# set server and ip
ceph config-key set mgr/dashboard/$name/server_addr $IP
ceph config-key set mgr/dashboard/$name/server_port $PORT
ceph config-key set mgr/dashboard/server_addr $IP
ceph config-key set mgr/dashboard/server_port $PORT



# osd pool size

$ceph osd pool get <pool-name> size   -->> it will prompt the " osd_pool_default_size "
$ceph osd pool get <pool-name> min_size    -->> it will prompt the " osd_pool_default_min_size "

if you want to change in runtime, trigger below command

$ceph osd pool set <pool-name> size <value>
$ceph osd pool set <pool-name> min_size <value>


# set device class
$ ceph osd crush rm-device-class osd.2 osd.3
done removing class of osd(s): 2,3
$ ceph osd crush set-device-class ssd osd.2 osd.3



# creating erasure coding

1. profile:
ceph osd erasure-code-profile set ec72profile k=7 m=2 crush-failure-domain=host crush-device-class=hdd

2. create our erasure coded pool
ceph osd pool create ec72pool 1024 erasure ec72profile

3. Next, we enable overwrites on the new pool (so that it can be used for RBD or CephFS):
ceph osd pool set ec72pool allow_ec_overwrites true

4. enable pool for cephfs
ceph osd pool application enable ec72pool cephfs


# get erasure profiles
ceph osd erasure-code-profile ls