http://download.ceph.com/tarballs/

http://docs.ceph.com/docs/master/man/8/ceph/


# show version
ceph version


# status
ceph -s


# health
ceph -w
ceph health


#  If you do not want CRUSH to automatically rebalance the cluster as you stop OSDs for maintenance, set the cluster to noout first
#  ( more flags to set: full|pause|noup|nodown|noout|noin|nobackfill|norebalance|norecover|noscrub|nodeep-scrub|notieragent|sortbitwise|require_jewel_osds )
ceph osd set noout
ceph osd unset noout
 

# list pools
ceph osd lspools


# disk usage
ceph df


# summarizes mds status
ceph mds stat -f json-pretty


# status of monitors
ceph mon_status -f json-pretty


# dump monitor state
ceph mon dump


# show osd tree
ceph osd tree


# list crush rule
ceph osd crush rule list -f json-pretty
ceph osd crush rule ls -f json-pretty


#shows current crush tunables
ceph osd crush show-tunables -f json-pretty


# shows the crush buckets and items in a tree view
ceph osd crush tree -f json-pretty


# sets crush tunables values to <profile>
ceph osd crush tunables legacy|argonaut|bobtail|firefly|hammer|optimal|default


# df shows OSD utilization
ceph osd df {plain|tree}


# get runtime config
# ceph daemon {daemon-type}.{id} config show | less
ceph daemon mon.cephconf01fra config show


# inject config
ceph tell {daemon-type}.{id or *} injectargs --{name} {value} [--{name} {value}]
ceph tell osd.0 injectargs --debug-osd 20 --debug-ms 1






http://docs.ceph.com/docs/master/rados/operations/crush-map/#warning-when-tunables-are-non-optimal

# if tunables are not optimal, ceph will complain ... requires at least kernel 4.x !
ceph osd crush tunables optimal
ceph osd crush tunables legacy




# remove host from bucket
# ceph osd crush remove {bucket-name}
ceph --cluster ceph-test osd crush remove $(hostname -s)



# list all "users"
ceph auth list


# get "user" caps
ceph auth get client.admin


# list erasure profiles
ceph osd erasure-code-profile ls


# import user
#The ceph storage cluster will add new users, their keys and their capabilities and will update existing users, their keys and their capabilities. 
ceph auth import -i /etc/ceph/ceph.keyring


# update client caps
ceph -c /etc/ceph/cephblocka01.conf auth caps client.pkgrepo mon 'allow r' mds 'allow' osd 'allow rwx pool=sftp_data'


# client caps
ceph auth get-or-create client.foo mon 'allow r' mds 'allow r, allow rw path=/mnt/' osd 'allow rw pool=sftp_data'
ceph auth get-or-create client.foo mon 'allow r' mds 'allow r' osd 'allow rw pool=sftp_data'


# ceph mds performance infos
ceph daemon mds.cephconf01fra perf dump mds


# after upgrade
ceph osd require-osd-release luminous


# set min version of clients
# jewel,kraken
ceph  osd set-require-min-compat-client luminous --yes-i-really-mean-it


# enable multimds
# 1st set the flag | 2nd set two active mds
ceph fs set FSNAME allow_multimds true
ceph fs set FSNAME max_mds 2


#mounting raw bluestore osds
ceph-objectstore-tool --op fuse --data-path /var/lib/ceph/osd/OSDNR--mountpount /mnt/foo


# memory usage of osds
ceph --cluster cephblocka01 daemon osd.6 dump_mempools


# block- or filestore ?
ceph --cluster cephblocka01 osd metadata $OSID | grep osd_objectstore


#current count of filestore vs bluestore
ceph --cluster cephblocka01 osd count-metadata osd_objectstore





[global]
client cache size = 81920


[osd]
#osd op threads = 2
osd journal size = 4GB
bluestore cache size ssd = 33GB
bluestore fsck on mount = true
bluestore block db size = 31GB
bluestore block wal size = 536870912